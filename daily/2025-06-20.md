
<br/>

**date** : 2025-06-20 <br/>
**title** : 생성형AI 활용을 위한 머신러닝/딥러닝 5<br/>
**tags** : [머신러닝, 지도학습, 분류, 로지스틱 회귀, KNN, 하이퍼파라미터] <br/>

<br/>

## 📌 오늘의 키워드

- 분류(Classification) - 로지스틱 회귀(Logistic Regression)
- K-최근접 이웃(KNN) - 거리 계산, 하이퍼파라미터 튜닝

<br/>

## 🧠 주요 개념 정리

### 1. 분류(Classification)
- 데이터를 특정 클래스나 범주로 분류하는 작업
- 지도학습(Supervised Learning)의 한 유형으로, 정답(레이블)을 기반으로 학습
- 예 : 이메일 스팸 필터링, 질병 진단 등

#### 분류의 특징
- **입력 데이터**: 다양한 특징(feature)을 가진 데이터
- **출력 데이터**: 분류된 클래스 (예: 스팸 / 정상 메일)
- **클래스 개수**
    - **이진 분류 (Binary Classification)** : 두 클래스 예측 (예: 0/1)
    - **다중 클래스 분류 (Multi-Class Classification)** : 세 개 이상의 클래스 예측

**분류의 작동 방식**
1. **데이터 수집 및 전처리** : 결측값 처리, 스케일링 등
2. **모델 학습** : 특징 + 정답 데이터를 활용
3. **모델 예측** : 학습한 모델이 새로운 데이터를 분류
4. **평가** : 예측 성능을 다양한 지표로 평가

### 2. 로지스틱 회귀(Logistic Regression)
- 분류 문제를 해결하는 지도학습 알고리즘 (특히 이진 분류에서 널리 사용)
- 입력 데이터를 바탕으로 각 클래스에 속할 확률을 예측

#### 작동 원리
- **선형 회귀 기반** : 먼저 선형 결합을 통해 예측값 𝑧 생성
- **시그모이드 함수 적용** : 𝑧를 시그모이드 함수에 통과시켜 확률값(0~1) 산출
- **클래스 결정** : 확률이 0.5 이상이면 1, 아니면 0으로 분류 (임계값 조정 가능)

#### 과정 요약
- **가설 설정** : ℎ𝜃(𝑥)=𝜎(𝑧) 형태
- **손실 함수** : 로그 손실 함수(Log Loss) 사용
- **모델 학습** : 경사 하강법으로 가중치 업데이트

### 3. 성능 평가 지표
- 혼동 행렬(Confusion Matrix)을 기반으로 다양한 지표 계산

| 항목          | 실제 Positive         | 실제 Negative         |
| ----------- | ------------------- | ------------------- |
| 예측 Positive | True Positive (TP)  | False Positive (FP) |
| 예측 Negative | False Negative (FN) | True Negative (TN)  |

#### 지표 정리

| 지표              | 설명                            | 장점             | 단점                |
| --------------- | ----------------------------- | -------------- | ----------------- |
| 정확도 (Accuracy)  | 전체 예측 중 맞춘 비율                 | 전체 성능 쉽게 파악 가능 | 클래스 불균형에 취약       |
| 정밀도 (Precision) | 예측한 Positive 중 실제 Positive 비율 | FP 줄이기에 유리     | FN 가능성 있음         |
| 재현율 (Recall)    | 실제 Positive 중 맞춘 비율           | FN 줄이기에 유리     | FP 가능성 있음         |
| F1-score        | 정밀도와 재현율의 조화 평균               | 두 지표의 균형 평가    | 클래스 불균형에 추가 분석 필요 |

#### 지표 선택 기준
- **Accuracy** : 클래스 균형일 때
- **Precision** : False Positive가 치명적인 경우 (예: 스팸 필터링)
- **Recall** : False Negative가 치명적인 경우 (예: 암 진단)
- **F1-score** : 정밀도 & 재현율 모두 중요한 경우 (예: 키워드 추출)

### 3. K-최근접 이웃(KNN)
- 지도 학습(Supervised Learning) 알고리즘 중 하나
- "가까운 데이터는 유사한 특성을 가진다"는 가정에 기반
- 분류(Classification)와 회귀(Regression) 모두에 사용 가능
- 학습 없이 실시간으로 예측 수행 → Lazy Learning 방식

#### 작동 원리
1. **훈련 데이터 준비**: 레이블이 있는 데이터를 준비  
2. **거리 계산**: 예측할 데이터와 훈련 데이터 간 거리 계산 (보통 유클리드 거리)  
3. **K개의 이웃 선택**: 가장 가까운 K개의 데이터를 선택  
4. **결과 결정**
   - **분류** : 다수결(Majority Voting)  
   - **회귀** : 평균값

#### 주요 특징
- **모델 학습 없음** : 훈련 데이터는 저장만 함  
- **단순성** : 수학적으로 간단  
- **응용 범위 넓음** : 분류와 회귀 모두 적용 가능

### 2. K 값의 선택과 영향

#### K 값이 작을 때
- **장점**: 세부 패턴 민감 → 복잡한 데이터에 적합  
- **단점**: 과적합 위험, 노이즈에 민감

#### K 값 선택 기준
- **데이터 크기**  
  - **작은 데이터셋**: K = 3~5  
  - **큰 데이터셋**: K = 15~20 이상  
- **홀수 추천**: 클래스가 2개일 경우 동률 방지를 위해  
- **교차 검증 사용**: 다양한 K를 시도하며 성능 비교

### 3. 모델 성능 향상 전략

| 항목 | 설명 |
|------|------|
| 데이터 전처리 | 스케일링, 이상치 제거, 특성 선택 등 |
| 거리 계산 방식 | 유클리드, 맨해튼, 민코프스키 등 |
| 하이퍼파라미터 튜닝 | 최적 K값 선택, 가중치 적용 등 |

<br/>

## 💻 실습 내용

### 실습 주제: 파이썬과 scikit-learn을 활용한 머신러닝 기초
1. 회귀 알고리즘 - 선형 회귀
- 학생 성적 데이터셋을 이용해 수치형/범주형 데이터 전처리(OneHotEncoder, Pipeline)
- 학습/테스트 데이터 분리, 모델 학습 및 예측
- MSE, R² 등 회귀 모델 평가

2. 분류 알고리즘 - 로지스틱 회귀
- 유방암 데이터셋을 활용한 이진 분류
- 학습/테스트 분리, 모델 학습 및 예측
- 정확도, 정밀도, 재현율, F1-score, 혼동 행렬, 분류 보고서 등 다양한 평가 지표 출력

3. KNN (K-최근접 이웃) 알고리즘
- Iris 데이터셋을 이용한 다중 클래스 분류
- 학습/테스트 분리, 모델 학습 및 예측
- 정확도, 분류 보고서 출력
- 새로운 데이터에 대한 예측 및 예측 결과의 클래스명 출력

<br/>

## 🐛 문제 & 해결

- 해당 없음

<br/>

## ❓ 궁금한 점

- 궁금한 점 1
- 궁금한 점 2

<br/>

## 🔗 참고 자료

- 없음 (교재/강의 기반 정리)

<br/>

## 🗂 관련 파일

📁 코드 : [`/project/`](../project/python/2025-06-20.ipynb) <br/>
🖼 스크린샷 : 해당 없음

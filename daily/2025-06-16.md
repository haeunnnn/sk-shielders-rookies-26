
<br/>

**date** : 2025-06-16 <br/>
**title** : 생성형AI 활용을 위한 머신러닝/딥러닝 1<br/>
**tags** : [requests, BeautifulSoup, Selenium, CSV/Excel/JSON 저장, SQLite] <br/>

<br/>

## 📌 오늘의 키워드

- 데이터 수집 - requests, BeautifulSoup, Selenium
- 데이터 저장 - CSV, Excel, JSON
- 데이터베이스 - SQLite

<br/>

## 🧠 주요 개념 정리

### 1. 데이터 수집

| 방법 | 설명 |
|------|-----|
| 웹 스크래핑 | 웹사이트에서 HTML 데이터를 추출하여 필요한 정보 수집<br>→ `BeautifulSoup` 사용 |
| API 활용 | 서버가 제공하는 API를 통해 데이터를 JSON 등 형식으로 요청/응답 받음<br>→ `requests` 사용 |
| 데이터베이스 | SQL이나 NoSQL을 통해 쿼리 기반으로 데이터 조회 |
| 공개 데이터셋 | Kaggle 등에서 다운로드 후 `pandas`로 불러와 사용 |
| IoT 데이터 | 센서 등에서 실시간 수집되는 스트리밍 데이터 |
| 소셜 미디어 | 각 플랫폼의 API를 통해 데이터 수집 |

---

#### requests 모듈을 활용한 데이터 수집
- 서버에 HTTP 요청을 보내 데이터를 받아옴
- GET, POST, PUT, DELETE 등의 메서드 지원

```python
import requests
response = requests.get('https://example.com')
```

#### BeautifulSoup을 활용한 HTML 데이터 수집
- HTML/XML 파싱을 통해 원하는 요소를 추출
- 기본 구조

``` python
from bs4 import BeautifulSoup
soup = BeautifulSoup(html, 'html.parser')

```

**주요 함수**

| 함수 | 설명 |
|------|-----|
| `find()` | 조건에 맞는 첫 번째 태그 반환 |
| `find_all()` | 조건에 맞는 모든 태그 리스트 반환 |
| `select()` | CSS 선택자 기반 요소 추출 |

#### Selenium을 활용한 동적 웹 페이지 크롤링
- Selenium : 브라우저를 자동으로 조작하는 라이브러리 (실제 사용자처럼 행동)
- 기본 구조

``` python
from selenium import webdriver
from selenium.webdriver.common.by import By
```

**주요 기능**

| 기능 | 설명 |
|------|-----|
| 브라우저 제어 | 열기/닫기, URL 이동, 새 탭 열기, 창 크기 조절 등 |
| 자바스크립트 실행 | `driver.execute_script()` |
| 페이지 로딩 대기 | 명시적/암시적 대기 지원 |

**주요 함수**

| 함수 | 설명 |
|------|-----|
| `find_element()`  | 단일 요소 탐색 |
| `find_elements()` | 여러 요소 탐색 |

**Locator 유형**

| By 클래스 | 설명 |
|------|-----|
| `By.ID` | id 속성으로 찾기 |
| `By.NAME` | name 속성 |
| `By.CLASS_NAME` | class 속성   |
| `By.TAG_NAME` | 태그 이름 |
| `By.XPATH` | XPath 경로 |
| `By.CSS_SELECTOR` | CSS 선택자 |

#### API를 활용한 데이터 수집

**REST API 구조**
- **HTTP 메서드**: `GET` (조회), `POST` (등록), `PUT` (수정), `DELETE` (삭제)
- **URL 구조**: 자원에 접근하기 위한 경로

**공공데이터 활용 단계**
1. **API 키 발급**: 공공데이터포털 등에서 인증 키 신청
2. **데이터 요청**: requests로 API 호출
3. **JSON 응답 처리**: response.json()으로 변환
4. **전처리**: 결측치 처리, 중복 제거, 형식 변환 등
5. **분석 및 시각화**: Pandas, Matplotlib 등 활용
6. **AI 모델 학습**: 피처 추출 후 모델 훈련

### 2. 데이터 저장 및 관리

#### CSV 파일 저장 / 불러오기

| 작업 | 코드 예시 |
|------|-----|
| 저장 | `writer = csv.writer(file)`<br>`writer.writerows(data)` |
| 불러오기 | `pd.read_csv('파일경로')`<br>`data.head()` |

#### Excel 파일 저장 / 불러오기

| 작업 | 코드 예시 |
| ---- | --------------------------- |
| 저장  | `df.to_excel('파일명.xlsx')`   |
| 불러오기 | `pd.read_excel('파일명.xlsx')` |

#### JSON 파일 저장 / 불러오기

| 작업 | 코드 예시 |
| ---- | ------------------------ |
| 저장   | `json.dump(data, file)`  |
| 불러오기 | `data = json.load(file)` |

#### 파일 형식 비교

| 형식 | 특징 |
|------|-----|
| CSV   | 경량, 구조화된 텍스트, 대용량 데이터에 적합 |
| Excel | 시트, 서식 포함, 인터페이스 친화적 |
| JSON  | 계층 구조 표현 가능, API 응답 데이터에 주로 사용됨 |

### 3. SQLite를 활용한 데이터베이스 관리
- SQLite는 서버가 필요 없는 경량 RDBMS로 파이썬에서 기본 제공

#### 주요 작업

| 작업 | 설명 |
|------|-----|
| DB 연결  | `sqlite3.connect('db파일명.db')` |
| 커서 생성  | `conn.cursor()` |
| 쿼리 실행 | `cursor.execute(쿼리문)` |
| DB 연결 종료 | `connection.close()` |

<br/>

## 💻 실습 내용

### 실습 주제 1: 파이썬을 통해 웹 크롤링해보기
- requests : 웹 페이지 요청 및 응답 데이터 가져오기
- BeautifulSoup4 : HTML 파싱, 태그/속성/텍스트 추출, find/find_all/select_one 등 다양한 선택자 활용법
- Selenium : 동적 웹 페이지 자동화 및 크롤링, 브라우저 조작, 검색 및 결과 추출

### 실습 주제 2: 파이썬을 통해 수집한 데이터를 다양한 형식으로 저장해보기
- CSV : csv 모듈로 파일 저장 및 pandas로 읽기
- Excel : pandas DataFrame을 엑셀로 저장/불러오기
- JSON : json 모듈로 파일 저장 및 읽기

### 실습 주제 2: 파이썬을 통해 SQLite 데이터베이스 관리해보기
- SQLite 연동 : 데이터베이스 연결/생성, 테이블 생성
- CRUD 실습 : 데이터 삽입(INSERT), 조회(SELECT), 수정(UPDATE), 삭제(DELETE) 쿼리 실행 및 결과 확인


<br/>

## 🐛 문제 & 해결

- 해당 없음

<br/>

## ❓ 궁금한 점

- 궁금한 점 1
- 궁금한 점 2

<br/>

## 🔗 참고 자료

- 없음 (교재/강의 기반 정리)

<br/>

## 🗂 관련 파일

📁 코드 : [`/project/`](../project/python/2025-06-16.ipynb) <br/>
🖼 스크린샷 : 해당 없음
